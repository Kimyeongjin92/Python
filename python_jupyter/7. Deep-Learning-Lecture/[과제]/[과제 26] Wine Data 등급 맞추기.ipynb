{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-encoding을 써야할거여~ 등급맞추기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/coalastudy/data-science-lv1/blob/master/week6/wine.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/wine.csv',header=None)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:11]\n",
    "Y = dataset[:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=11, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss',\n",
    "                               verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4352 samples, validate on 2145 samples\n",
      "Epoch 1/200\n",
      "4352/4352 [==============================] - 0s 108us/step - loss: 3.4430 - acc: 0.3336 - val_loss: 0.8489 - val_acc: 0.4359\n",
      "Epoch 2/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.7810 - acc: 0.4494 - val_loss: 0.7525 - val_acc: 0.4653\n",
      "Epoch 3/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.7181 - acc: 0.4637 - val_loss: 0.6394 - val_acc: 0.4746\n",
      "Epoch 4/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.7054 - acc: 0.4750 - val_loss: 1.0723 - val_acc: 0.3483\n",
      "Epoch 5/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.6937 - acc: 0.4710 - val_loss: 0.6209 - val_acc: 0.4587\n",
      "Epoch 6/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.6417 - acc: 0.4853 - val_loss: 0.6128 - val_acc: 0.4597\n",
      "Epoch 7/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.6349 - acc: 0.4952 - val_loss: 0.7439 - val_acc: 0.4858\n",
      "Epoch 8/200\n",
      "4352/4352 [==============================] - 0s 80us/step - loss: 0.6128 - acc: 0.5044 - val_loss: 0.5791 - val_acc: 0.5329\n",
      "Epoch 9/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.6284 - acc: 0.4933 - val_loss: 0.5744 - val_acc: 0.5319\n",
      "Epoch 10/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.6023 - acc: 0.5117 - val_loss: 0.6071 - val_acc: 0.4755\n",
      "Epoch 11/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.6184 - acc: 0.4972 - val_loss: 0.6234 - val_acc: 0.5184\n",
      "Epoch 12/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.6013 - acc: 0.4982 - val_loss: 0.9318 - val_acc: 0.3571\n",
      "Epoch 13/200\n",
      "4352/4352 [==============================] - 0s 65us/step - loss: 0.6222 - acc: 0.4979 - val_loss: 1.1895 - val_acc: 0.2834\n",
      "Epoch 14/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.6187 - acc: 0.4995 - val_loss: 0.5817 - val_acc: 0.4779\n",
      "Epoch 15/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5851 - acc: 0.5218 - val_loss: 0.5898 - val_acc: 0.5389\n",
      "Epoch 16/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.6278 - acc: 0.4993 - val_loss: 0.9441 - val_acc: 0.4005\n",
      "Epoch 17/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5846 - acc: 0.5163 - val_loss: 0.5230 - val_acc: 0.5324\n",
      "Epoch 18/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5937 - acc: 0.5092 - val_loss: 0.5299 - val_acc: 0.5291\n",
      "Epoch 19/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5765 - acc: 0.5161 - val_loss: 0.5823 - val_acc: 0.4755\n",
      "Epoch 20/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5623 - acc: 0.5227 - val_loss: 0.5849 - val_acc: 0.4811\n",
      "Epoch 21/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5717 - acc: 0.5149 - val_loss: 0.5350 - val_acc: 0.4984\n",
      "Epoch 22/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5956 - acc: 0.5092 - val_loss: 0.7058 - val_acc: 0.5077\n",
      "Epoch 23/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5675 - acc: 0.5136 - val_loss: 0.6627 - val_acc: 0.4443\n",
      "Epoch 24/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5858 - acc: 0.5154 - val_loss: 0.5637 - val_acc: 0.4811\n",
      "Epoch 25/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5787 - acc: 0.5129 - val_loss: 0.9481 - val_acc: 0.3953\n",
      "Epoch 26/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5845 - acc: 0.5163 - val_loss: 0.5412 - val_acc: 0.5338\n",
      "Epoch 27/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5835 - acc: 0.5117 - val_loss: 0.6530 - val_acc: 0.4545\n",
      "Epoch 28/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5634 - acc: 0.5253 - val_loss: 0.5306 - val_acc: 0.5179\n",
      "Epoch 29/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5709 - acc: 0.5260 - val_loss: 0.5409 - val_acc: 0.4867\n",
      "Epoch 30/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5690 - acc: 0.5234 - val_loss: 0.5345 - val_acc: 0.4993\n",
      "Epoch 31/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5761 - acc: 0.5115 - val_loss: 0.5430 - val_acc: 0.5529\n",
      "Epoch 32/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5616 - acc: 0.5239 - val_loss: 0.5127 - val_acc: 0.5310\n",
      "Epoch 33/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5518 - acc: 0.5250 - val_loss: 0.6111 - val_acc: 0.4648\n",
      "Epoch 34/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5761 - acc: 0.5140 - val_loss: 0.5424 - val_acc: 0.4788\n",
      "Epoch 35/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5785 - acc: 0.5119 - val_loss: 0.5361 - val_acc: 0.5413\n",
      "Epoch 36/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5604 - acc: 0.5126 - val_loss: 0.5458 - val_acc: 0.5399\n",
      "Epoch 37/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5658 - acc: 0.5149 - val_loss: 0.7199 - val_acc: 0.4345\n",
      "Epoch 38/200\n",
      "4352/4352 [==============================] - 0s 78us/step - loss: 0.5713 - acc: 0.5165 - val_loss: 0.5307 - val_acc: 0.5324\n",
      "Epoch 39/200\n",
      "4352/4352 [==============================] - 0s 77us/step - loss: 0.5560 - acc: 0.5202 - val_loss: 0.6009 - val_acc: 0.4629\n",
      "Epoch 40/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5541 - acc: 0.5296 - val_loss: 0.5490 - val_acc: 0.4890\n",
      "Epoch 41/200\n",
      "4352/4352 [==============================] - 0s 83us/step - loss: 0.5561 - acc: 0.5269 - val_loss: 0.5635 - val_acc: 0.4844\n",
      "Epoch 42/200\n",
      "4352/4352 [==============================] - 0s 76us/step - loss: 0.5556 - acc: 0.5207 - val_loss: 0.5529 - val_acc: 0.4839\n",
      "Epoch 43/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5577 - acc: 0.5239 - val_loss: 0.5273 - val_acc: 0.5128\n",
      "Epoch 44/200\n",
      "4352/4352 [==============================] - 0s 76us/step - loss: 0.5512 - acc: 0.5154 - val_loss: 0.5782 - val_acc: 0.4797\n",
      "Epoch 45/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5655 - acc: 0.5237 - val_loss: 0.5772 - val_acc: 0.4737\n",
      "Epoch 46/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5517 - acc: 0.5188 - val_loss: 0.5294 - val_acc: 0.5343\n",
      "Epoch 47/200\n",
      "4352/4352 [==============================] - 0s 78us/step - loss: 0.5553 - acc: 0.5168 - val_loss: 0.5433 - val_acc: 0.4848\n",
      "Epoch 48/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5656 - acc: 0.5145 - val_loss: 0.5247 - val_acc: 0.5077\n",
      "Epoch 49/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5644 - acc: 0.5136 - val_loss: 0.6550 - val_acc: 0.4541\n",
      "Epoch 50/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5466 - acc: 0.5303 - val_loss: 0.5802 - val_acc: 0.5413\n",
      "Epoch 51/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5599 - acc: 0.5234 - val_loss: 0.5266 - val_acc: 0.5380\n",
      "Epoch 52/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5568 - acc: 0.5253 - val_loss: 0.5456 - val_acc: 0.4867\n",
      "Epoch 53/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5647 - acc: 0.5129 - val_loss: 0.5780 - val_acc: 0.5408\n",
      "Epoch 54/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5534 - acc: 0.5149 - val_loss: 0.5296 - val_acc: 0.4988\n",
      "Epoch 55/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5454 - acc: 0.5230 - val_loss: 0.5201 - val_acc: 0.5259\n",
      "Epoch 56/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5471 - acc: 0.5317 - val_loss: 0.5340 - val_acc: 0.5021\n",
      "Epoch 57/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5495 - acc: 0.5319 - val_loss: 0.5223 - val_acc: 0.5175\n",
      "Epoch 58/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5614 - acc: 0.5276 - val_loss: 0.5948 - val_acc: 0.4732\n",
      "Epoch 59/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5486 - acc: 0.5200 - val_loss: 0.6198 - val_acc: 0.4583\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5438 - acc: 0.5287 - val_loss: 0.9212 - val_acc: 0.4182\n",
      "Epoch 61/200\n",
      "4352/4352 [==============================] - ETA: 0s - loss: 0.5524 - acc: 0.521 - 0s 66us/step - loss: 0.5486 - acc: 0.5223 - val_loss: 0.5228 - val_acc: 0.5403\n",
      "Epoch 62/200\n",
      "4352/4352 [==============================] - 0s 69us/step - loss: 0.5466 - acc: 0.5280 - val_loss: 0.6445 - val_acc: 0.5156\n",
      "Epoch 63/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5482 - acc: 0.5230 - val_loss: 0.5396 - val_acc: 0.5538\n",
      "Epoch 64/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5613 - acc: 0.5269 - val_loss: 0.7066 - val_acc: 0.4331\n",
      "Epoch 65/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5466 - acc: 0.5218 - val_loss: 0.5492 - val_acc: 0.5520\n",
      "Epoch 66/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5486 - acc: 0.5216 - val_loss: 0.9212 - val_acc: 0.4145\n",
      "Epoch 67/200\n",
      "4352/4352 [==============================] - 0s 75us/step - loss: 0.5548 - acc: 0.5163 - val_loss: 0.5665 - val_acc: 0.4825\n",
      "Epoch 68/200\n",
      "4352/4352 [==============================] - 0s 79us/step - loss: 0.5506 - acc: 0.5198 - val_loss: 0.6752 - val_acc: 0.4336\n",
      "Epoch 69/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5449 - acc: 0.5223 - val_loss: 0.5878 - val_acc: 0.4741\n",
      "Epoch 70/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5553 - acc: 0.5177 - val_loss: 0.5472 - val_acc: 0.4848\n",
      "Epoch 71/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5409 - acc: 0.5202 - val_loss: 0.6421 - val_acc: 0.4527\n",
      "Epoch 72/200\n",
      "4352/4352 [==============================] - 0s 69us/step - loss: 0.5488 - acc: 0.5207 - val_loss: 0.5191 - val_acc: 0.5338\n",
      "Epoch 73/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5461 - acc: 0.5285 - val_loss: 0.6138 - val_acc: 0.5231\n",
      "Epoch 74/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5613 - acc: 0.5250 - val_loss: 0.5579 - val_acc: 0.4853\n",
      "Epoch 75/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5410 - acc: 0.5248 - val_loss: 0.5425 - val_acc: 0.5520\n",
      "Epoch 76/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5385 - acc: 0.5234 - val_loss: 0.5107 - val_acc: 0.5371\n",
      "Epoch 77/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5415 - acc: 0.5271 - val_loss: 0.8479 - val_acc: 0.3674\n",
      "Epoch 78/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5363 - acc: 0.5296 - val_loss: 0.5160 - val_acc: 0.5226\n",
      "Epoch 79/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5392 - acc: 0.5294 - val_loss: 0.5212 - val_acc: 0.5235\n",
      "Epoch 80/200\n",
      "4352/4352 [==============================] - 0s 75us/step - loss: 0.5356 - acc: 0.5202 - val_loss: 0.5164 - val_acc: 0.5347\n",
      "Epoch 81/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5391 - acc: 0.5260 - val_loss: 0.5785 - val_acc: 0.4811\n",
      "Epoch 82/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5324 - acc: 0.5290 - val_loss: 0.5418 - val_acc: 0.5464\n",
      "Epoch 83/200\n",
      "4352/4352 [==============================] - 0s 77us/step - loss: 0.5315 - acc: 0.5262 - val_loss: 0.5369 - val_acc: 0.4956\n",
      "Epoch 84/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5430 - acc: 0.5393 - val_loss: 0.5152 - val_acc: 0.5193\n",
      "Epoch 85/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5362 - acc: 0.5326 - val_loss: 0.6380 - val_acc: 0.4573\n",
      "Epoch 86/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5337 - acc: 0.5198 - val_loss: 0.5260 - val_acc: 0.5473\n",
      "Epoch 87/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5309 - acc: 0.5352 - val_loss: 0.5989 - val_acc: 0.4732\n",
      "Epoch 88/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5414 - acc: 0.5273 - val_loss: 0.5183 - val_acc: 0.5166\n",
      "Epoch 89/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5313 - acc: 0.5393 - val_loss: 0.5270 - val_acc: 0.5263\n",
      "Epoch 90/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5477 - acc: 0.5278 - val_loss: 0.5653 - val_acc: 0.4793\n",
      "Epoch 91/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5311 - acc: 0.5331 - val_loss: 0.5493 - val_acc: 0.4928\n",
      "Epoch 92/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5307 - acc: 0.5342 - val_loss: 0.6034 - val_acc: 0.4695\n",
      "Epoch 93/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5332 - acc: 0.5352 - val_loss: 0.5378 - val_acc: 0.5389\n",
      "Epoch 94/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5451 - acc: 0.5294 - val_loss: 0.5371 - val_acc: 0.4960\n",
      "Epoch 95/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5255 - acc: 0.5301 - val_loss: 0.5588 - val_acc: 0.4909\n",
      "Epoch 96/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5412 - acc: 0.5225 - val_loss: 0.5357 - val_acc: 0.4918\n",
      "Epoch 97/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5353 - acc: 0.5255 - val_loss: 0.5591 - val_acc: 0.4867\n",
      "Epoch 98/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5251 - acc: 0.5308 - val_loss: 0.5348 - val_acc: 0.5296\n",
      "Epoch 99/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5363 - acc: 0.5306 - val_loss: 0.5487 - val_acc: 0.4998\n",
      "Epoch 100/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5372 - acc: 0.5214 - val_loss: 0.5396 - val_acc: 0.4904\n",
      "Epoch 101/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5267 - acc: 0.5278 - val_loss: 0.5358 - val_acc: 0.5030\n",
      "Epoch 102/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5352 - acc: 0.5310 - val_loss: 0.5345 - val_acc: 0.5427\n",
      "Epoch 103/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5352 - acc: 0.5273 - val_loss: 0.6142 - val_acc: 0.4667\n",
      "Epoch 104/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5307 - acc: 0.5290 - val_loss: 0.5944 - val_acc: 0.4751\n",
      "Epoch 105/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5367 - acc: 0.5290 - val_loss: 0.5307 - val_acc: 0.5207\n",
      "Epoch 106/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5302 - acc: 0.5292 - val_loss: 0.6512 - val_acc: 0.4434\n",
      "Epoch 107/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5221 - acc: 0.5423 - val_loss: 0.5539 - val_acc: 0.4895\n",
      "Epoch 108/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5357 - acc: 0.5296 - val_loss: 0.5257 - val_acc: 0.5133\n",
      "Epoch 109/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5251 - acc: 0.5372 - val_loss: 0.5726 - val_acc: 0.4844\n",
      "Epoch 110/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5295 - acc: 0.5262 - val_loss: 0.7085 - val_acc: 0.4303\n",
      "Epoch 111/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5322 - acc: 0.5306 - val_loss: 0.5262 - val_acc: 0.5315\n",
      "Epoch 112/200\n",
      "4352/4352 [==============================] - 0s 75us/step - loss: 0.5317 - acc: 0.5283 - val_loss: 0.5394 - val_acc: 0.4988\n",
      "Epoch 113/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5320 - acc: 0.5257 - val_loss: 0.7815 - val_acc: 0.4084\n",
      "Epoch 114/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5342 - acc: 0.5335 - val_loss: 0.6741 - val_acc: 0.4452\n",
      "Epoch 115/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5263 - acc: 0.5322 - val_loss: 0.5697 - val_acc: 0.4797\n",
      "Epoch 116/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5267 - acc: 0.5368 - val_loss: 0.5302 - val_acc: 0.5058\n",
      "Epoch 117/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5275 - acc: 0.5381 - val_loss: 0.6047 - val_acc: 0.4704\n",
      "Epoch 118/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5238 - acc: 0.5352 - val_loss: 0.7653 - val_acc: 0.4145\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4352/4352 [==============================] - 0s 69us/step - loss: 0.5262 - acc: 0.5308 - val_loss: 0.5209 - val_acc: 0.5408\n",
      "Epoch 120/200\n",
      "4352/4352 [==============================] - 0s 69us/step - loss: 0.5286 - acc: 0.5315 - val_loss: 0.5190 - val_acc: 0.5175\n",
      "Epoch 121/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5238 - acc: 0.5278 - val_loss: 0.5856 - val_acc: 0.5427\n",
      "Epoch 122/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5248 - acc: 0.5365 - val_loss: 0.6559 - val_acc: 0.4499\n",
      "Epoch 123/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5228 - acc: 0.5292 - val_loss: 0.5221 - val_acc: 0.5375\n",
      "Epoch 124/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5303 - acc: 0.5356 - val_loss: 0.5297 - val_acc: 0.5427\n",
      "Epoch 125/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5240 - acc: 0.5377 - val_loss: 0.6003 - val_acc: 0.4713\n",
      "Epoch 126/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5309 - acc: 0.5280 - val_loss: 0.6602 - val_acc: 0.4457\n",
      "Epoch 127/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5393 - acc: 0.5237 - val_loss: 0.5413 - val_acc: 0.5403\n",
      "Epoch 128/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5314 - acc: 0.5232 - val_loss: 0.5198 - val_acc: 0.5287\n",
      "Epoch 129/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5271 - acc: 0.5257 - val_loss: 0.5271 - val_acc: 0.5058\n",
      "Epoch 130/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5277 - acc: 0.5296 - val_loss: 0.5646 - val_acc: 0.4844\n",
      "Epoch 131/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5238 - acc: 0.5388 - val_loss: 0.5251 - val_acc: 0.5100\n",
      "Epoch 132/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5276 - acc: 0.5329 - val_loss: 0.5226 - val_acc: 0.5152\n",
      "Epoch 133/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5262 - acc: 0.5338 - val_loss: 0.5653 - val_acc: 0.4881\n",
      "Epoch 134/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5297 - acc: 0.5345 - val_loss: 0.5224 - val_acc: 0.5138\n",
      "Epoch 135/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5251 - acc: 0.5306 - val_loss: 0.5356 - val_acc: 0.4918\n",
      "Epoch 136/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5223 - acc: 0.5347 - val_loss: 0.5437 - val_acc: 0.5403\n",
      "Epoch 137/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5270 - acc: 0.5308 - val_loss: 0.5702 - val_acc: 0.4802\n",
      "Epoch 138/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5234 - acc: 0.5381 - val_loss: 0.5246 - val_acc: 0.5152\n",
      "Epoch 139/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5229 - acc: 0.5398 - val_loss: 0.5353 - val_acc: 0.4970\n",
      "Epoch 140/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5286 - acc: 0.5386 - val_loss: 0.5222 - val_acc: 0.5152\n",
      "Epoch 141/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5213 - acc: 0.5400 - val_loss: 0.6093 - val_acc: 0.4685\n",
      "Epoch 142/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5282 - acc: 0.5313 - val_loss: 0.5364 - val_acc: 0.5212\n",
      "Epoch 143/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5365 - acc: 0.5246 - val_loss: 0.5513 - val_acc: 0.5413\n",
      "Epoch 144/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5273 - acc: 0.5267 - val_loss: 0.5221 - val_acc: 0.5193\n",
      "Epoch 145/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5217 - acc: 0.5379 - val_loss: 0.5441 - val_acc: 0.4872\n",
      "Epoch 146/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5213 - acc: 0.5347 - val_loss: 0.5436 - val_acc: 0.5040\n",
      "Epoch 147/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5232 - acc: 0.5375 - val_loss: 0.5262 - val_acc: 0.5152\n",
      "Epoch 148/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5238 - acc: 0.5324 - val_loss: 0.5949 - val_acc: 0.4709\n",
      "Epoch 149/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5191 - acc: 0.5418 - val_loss: 0.5493 - val_acc: 0.4858\n",
      "Epoch 150/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5231 - acc: 0.5303 - val_loss: 0.5397 - val_acc: 0.5049\n",
      "Epoch 151/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5200 - acc: 0.5434 - val_loss: 0.5410 - val_acc: 0.5063\n",
      "Epoch 152/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5261 - acc: 0.5400 - val_loss: 0.5823 - val_acc: 0.4695\n",
      "Epoch 153/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5185 - acc: 0.5443 - val_loss: 0.5412 - val_acc: 0.4942\n",
      "Epoch 154/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5193 - acc: 0.5409 - val_loss: 0.5303 - val_acc: 0.5422\n",
      "Epoch 155/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5133 - acc: 0.5450 - val_loss: 0.5694 - val_acc: 0.4858\n",
      "Epoch 156/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5234 - acc: 0.5310 - val_loss: 0.5409 - val_acc: 0.4853\n",
      "Epoch 157/200\n",
      "4352/4352 [==============================] - 0s 76us/step - loss: 0.5216 - acc: 0.5375 - val_loss: 0.7129 - val_acc: 0.4163\n",
      "Epoch 158/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5199 - acc: 0.5363 - val_loss: 0.5296 - val_acc: 0.5268\n",
      "Epoch 159/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5180 - acc: 0.5301 - val_loss: 0.5544 - val_acc: 0.4834\n",
      "Epoch 160/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5128 - acc: 0.5398 - val_loss: 0.5611 - val_acc: 0.4844\n",
      "Epoch 161/200\n",
      "4352/4352 [==============================] - 0s 76us/step - loss: 0.5162 - acc: 0.5432 - val_loss: 0.6351 - val_acc: 0.4573\n",
      "Epoch 162/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5226 - acc: 0.5391 - val_loss: 0.5371 - val_acc: 0.5002\n",
      "Epoch 163/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5208 - acc: 0.5368 - val_loss: 0.5834 - val_acc: 0.4839\n",
      "Epoch 164/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5177 - acc: 0.5377 - val_loss: 0.5326 - val_acc: 0.4951\n",
      "Epoch 165/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5195 - acc: 0.5349 - val_loss: 0.5253 - val_acc: 0.5105\n",
      "Epoch 166/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5218 - acc: 0.5372 - val_loss: 0.5405 - val_acc: 0.4862\n",
      "Epoch 167/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5229 - acc: 0.5430 - val_loss: 0.5306 - val_acc: 0.5310\n",
      "Epoch 168/200\n",
      "4352/4352 [==============================] - 0s 76us/step - loss: 0.5233 - acc: 0.5292 - val_loss: 0.5361 - val_acc: 0.4951\n",
      "Epoch 169/200\n",
      "4352/4352 [==============================] - 0s 81us/step - loss: 0.5140 - acc: 0.5450 - val_loss: 0.5194 - val_acc: 0.5324\n",
      "Epoch 170/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5225 - acc: 0.5372 - val_loss: 0.6977 - val_acc: 0.4270\n",
      "Epoch 171/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5338 - acc: 0.5273 - val_loss: 0.5521 - val_acc: 0.4946\n",
      "Epoch 172/200\n",
      "4352/4352 [==============================] - 0s 78us/step - loss: 0.5187 - acc: 0.5356 - val_loss: 0.5288 - val_acc: 0.5156\n",
      "Epoch 173/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5228 - acc: 0.5349 - val_loss: 0.5848 - val_acc: 0.4765\n",
      "Epoch 174/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5186 - acc: 0.5418 - val_loss: 0.5595 - val_acc: 0.4821\n",
      "Epoch 175/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5209 - acc: 0.5349 - val_loss: 0.5271 - val_acc: 0.5291\n",
      "Epoch 176/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5210 - acc: 0.5294 - val_loss: 0.5501 - val_acc: 0.4909\n",
      "Epoch 177/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5140 - acc: 0.5411 - val_loss: 0.5790 - val_acc: 0.4811\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5127 - acc: 0.5375 - val_loss: 0.5246 - val_acc: 0.5030\n",
      "Epoch 179/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5142 - acc: 0.5402 - val_loss: 0.5538 - val_acc: 0.4811\n",
      "Epoch 180/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5176 - acc: 0.5372 - val_loss: 0.5701 - val_acc: 0.4821\n",
      "Epoch 181/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5126 - acc: 0.5365 - val_loss: 0.5331 - val_acc: 0.4946\n",
      "Epoch 182/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5209 - acc: 0.5326 - val_loss: 0.5467 - val_acc: 0.4876\n",
      "Epoch 183/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5282 - acc: 0.5340 - val_loss: 0.6253 - val_acc: 0.4615\n",
      "Epoch 184/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5170 - acc: 0.5372 - val_loss: 0.5415 - val_acc: 0.4988\n",
      "Epoch 185/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5201 - acc: 0.5358 - val_loss: 0.6414 - val_acc: 0.4452\n",
      "Epoch 186/200\n",
      "4352/4352 [==============================] - 0s 66us/step - loss: 0.5165 - acc: 0.5361 - val_loss: 0.5490 - val_acc: 0.4853\n",
      "Epoch 187/200\n",
      "4352/4352 [==============================] - 0s 71us/step - loss: 0.5203 - acc: 0.5391 - val_loss: 0.5701 - val_acc: 0.4788\n",
      "Epoch 188/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5185 - acc: 0.5301 - val_loss: 0.5354 - val_acc: 0.5273\n",
      "Epoch 189/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5132 - acc: 0.5398 - val_loss: 0.5263 - val_acc: 0.5096\n",
      "Epoch 190/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5184 - acc: 0.5352 - val_loss: 0.5253 - val_acc: 0.5110\n",
      "Epoch 191/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5162 - acc: 0.5388 - val_loss: 0.5668 - val_acc: 0.4825\n",
      "Epoch 192/200\n",
      "4352/4352 [==============================] - 0s 70us/step - loss: 0.5154 - acc: 0.5400 - val_loss: 0.5307 - val_acc: 0.5333\n",
      "Epoch 193/200\n",
      "4352/4352 [==============================] - 0s 67us/step - loss: 0.5207 - acc: 0.5333 - val_loss: 0.5432 - val_acc: 0.5114\n",
      "Epoch 194/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5178 - acc: 0.5388 - val_loss: 0.6016 - val_acc: 0.4648\n",
      "Epoch 195/200\n",
      "4352/4352 [==============================] - 0s 69us/step - loss: 0.5217 - acc: 0.5423 - val_loss: 0.5399 - val_acc: 0.5030\n",
      "Epoch 196/200\n",
      "4352/4352 [==============================] - 0s 68us/step - loss: 0.5172 - acc: 0.5434 - val_loss: 0.5460 - val_acc: 0.4998\n",
      "Epoch 197/200\n",
      "4352/4352 [==============================] - 0s 75us/step - loss: 0.5172 - acc: 0.5386 - val_loss: 0.5393 - val_acc: 0.5016\n",
      "Epoch 198/200\n",
      "4352/4352 [==============================] - 0s 72us/step - loss: 0.5140 - acc: 0.5450 - val_loss: 0.5867 - val_acc: 0.4732\n",
      "Epoch 199/200\n",
      "4352/4352 [==============================] - 0s 74us/step - loss: 0.5191 - acc: 0.5430 - val_loss: 0.5595 - val_acc: 0.4904\n",
      "Epoch 200/200\n",
      "4352/4352 [==============================] - 0s 73us/step - loss: 0.5111 - acc: 0.5455 - val_loss: 0.6392 - val_acc: 0.4545\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, validation_split=0.33, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제등급:6.000,예상등급:5.620\n",
      "실제등급:6.000,예상등급:6.862\n",
      "실제등급:6.000,예상등급:5.849\n",
      "실제등급:5.000,예상등급:4.393\n",
      "실제등급:8.000,예상등급:6.478\n",
      "실제등급:5.000,예상등급:5.888\n",
      "실제등급:6.000,예상등급:6.591\n",
      "실제등급:6.000,예상등급:5.702\n",
      "실제등급:6.000,예상등급:5.923\n",
      "실제등급:6.000,예상등급:6.613\n"
     ]
    }
   ],
   "source": [
    "# 모델의 학습이 어느 정도 되었는지 확인하기 위해 \n",
    "# 예측 값과 실제 값을 비교하는 부분을 추가한다.\n",
    "\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "for i in range(10):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제등급:{:.3f},예상등급:{:.3f}\".format(label,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_acc에 학습셋으로 측정한 정확도의 값을 저장\n",
    "y_acc = history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c= \"blue\",markersize=3)\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
